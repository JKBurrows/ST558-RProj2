---
title: "ST558 R Project II"
author: "Mana Azizsoltani"
date: "October 16, 2020"
output: rmarkdown::github_document
params:  
  day: "monday"
---

```{r setup, include=FALSE, message=FALSE}
# Set seed for reproducibility
set.seed(445)

# Load necessary libraries
library(knitr)
library(gridExtra)
library(rmarkdown)
library(caret)
library(tidyverse)
library(class)
library(randomForest)
library(gbm)
library(corrplot)
library(klaR)
```

# Introduction
## Online News Data Set
The online news data set that we will be working with for this project was found on the UCI Machine Learning Repository. The data itself comes from the popular online news website, Mashable. Each observation of the data set represents a single article released by Mashable during the period from January 7, 2013 and January 7, 2015. Each of the 39644 articles has 60 attributes that describes the different aspects of each article.

## Variable Descriptions
The variables that I will be using are the following (more on selection later):

7. num_hrefs: Number of links  
14. data_channel_is_entertainment: Is data channel 'Entertainment'?  
16. data_channel_is_socmed: Is data channel 'Social Media'?  
17. data_channel_is_tech: Is data channel 'Tech'?  
18. data_channel_is_world: Is data channel 'World'?  
19. kw_min_min: Worst keyword (min. shares)  
25. kw_min_avg: Avg. keyword (min. shares)  
26. kw_max_avg: Avg. keyword (max. shares)  
27. kw_avg_avg: Avg. keyword (avg. shares)  
36. weekday_is_saturday: Was the article published on a Saturday?  
37. weekday_is_sunday: Was the article published on a Sunday?  
38. is_weekend: Was the article published on the weekend?  
39. LDA_00: Closeness to LDA topic 0  
40. LDA_01: Closeness to LDA topic 1  
41. LDA_02: Closeness to LDA topic 2  
43. LDA_04: Closeness to LDA topic 4  
44. global_subjectivity: Text subjectivity  
45. global_sentiment_polarity: Text sentiment polarity  
49. rate_negative_words: Rate of negative words among non-neutral tokens  
56. title_subjectivity: Title subjectivity  
60. shares: Number of shares (target)  

## Purpose and Methods
The purpose of this analysis is to predict the number of shares of a particular article using tree-based machine learning techniques. In particular, we will be using a normal (non-ensemble) regression tree and a boosted tree model to attempt to predict the number of shares. This process will be done for each day of the week.

# Data
## Reading in the Data
First things first, we need to read in the data and filter the data for `r str_to_title(params$day)`.  
```{r, message=FALSE}
# Read in data set
dat <- read_csv("OnlineNewsPopularity.csv") %>% dplyr::select(-url)

# Subset the data set by day
dayvar <- as.name(paste0("weekday_is_", params$day))
day.dat <- dat %>% filter((!!sym(dayvar)) == 1)
```

## Variable Selection
After reading the article attached to the UCI Machine Learning website where we got the data, I decided to use the same technique as Dr. Ren and Dr. Yang as discussed in [their paper](http://cs229.stanford.edu/proj2015/328_report.pdf). They calculated the Fisher score for each feature and selected the 20 with the highest Fisher scores. The Fisher score (for data with two classes) for the $j^{\text{th}}$ feature is given by:
$$F(j) =  \frac{(\bar{x}^1_j - \bar{x}^2_j)^2}{(s^1_j)^2 + (s^2_j)^2},$$
where
$$(s^k_j)^2 = \sum_{x \in X^k}(x_j - \bar{x}^k_j)^2$$

The top variables with the highest Fisher scores were the following:
```{r fscores, echo=FALSE}
knitr::include_graphics("/Users/mana010/Downloads/fscores.png")
```
All that being said, I used these 20 variables as my predictors in my models. This is also consistent with what we learned in the lectures about how many predictors to use in a regression tree, which is $\frac{1}{3}(\text{# of predictors})$ = 20 in this case.

## Data Partitioning
Before creating the models, we must split the data into a training and test data set in order to later evaluate the model's prediction accuracy. In this case we will be using a 70/30 split, training the data on the 70% and testing the trained models on the 30%.  
```{r datasplit}
vars <- c(7, 14, 16:18, 26:27, 36:39, 41, 43:44, 60)
index <- createDataPartition(day.dat$shares, p = .7, list = F) %>% as.vector()
train <- day.dat[index,vars]
test <- day.dat[-index,vars]
```

# Summarization
To get an idea of what we are working with when considering this data, we want to get some summaries of the data. Before looking at the training data, I wanted to see how much of the data pertained to each day.
```{r plot1, echo=FALSE}
counts <- round(colSums(dat[,31:37])/nrow(dat)*100, 1)
DOW <- c("Monday", "Tuesday", "Wednesday", "Thursday",
         "Friday", "Saturday", "Sunday")
df.counts <- cbind.data.frame("Day" = DOW, "Count" = counts)
df.counts$Day <- factor(df.counts$Day, levels = df.counts$Day)

ggplot(data = df.counts, aes(x = Day, y = counts)) + geom_bar(stat = "identity", fill = "#CC0000") + 
  labs(x = "Day of Week", y = "Percentage", title = "Percentage of Data for Each Day") + 
  scale_y_continuous(limits = c(0, 22)) + geom_text(aes(label=counts), vjust = 1.6, color = "white", size = 4)

```

Next, I went ahead and looked at summaries of the four variables with the highest Fisher Scores. The case of the quantitative variables, I plotted them on a scatter plot against `shares`, and for the qualitative variables, I just whipped up a bar plot to get a breakdown of the training data.
```{r sctrs, message=FALSE, echo=FALSE, warning=FALSE}
train2 <- train %>% filter(shares < max(shares))
max(train$shares)

gg1 <- ggplot(data = train2, aes(x = (kw_avg_avg-mean(kw_avg_avg))/sd(kw_avg_avg),
                                 y = (shares-mean(shares))/sd(shares))) + geom_point() +
         labs(y = "Shares (Standardized)", x = "Avg. Keyword (Standardized)", title = "Shares vs. Avg. Keyword")

gg2 <- ggplot(data = train2, aes(x = LDA_02, y = shares)) + geom_point() + 
         labs(x = "Closeness to LDA topic 2", y = "Shares", title = "Shares vs. LDA_02")

gg3 <- ggplot(data = train, aes(x = data_channel_is_world)) + geom_bar() + 
         scale_x_discrete(limits = c(0,1), labels = c("Not World Channel", "World Channel")) + 
         labs(x = "Is data channel 'World'?", y = "Count", title = "Is World Channel Breakdown")

gg4 <- ggplot(data = train, aes(x = data_channel_is_socmed)) + geom_bar() + 
         scale_x_discrete(limits = c(0,1), labels = c("Not S.M. Channel", "S.M. Channel")) + 
         labs(x = "Is data channel 'Social Media'?", y = "Count", title = "Is Social Media Channel Breakdown")

grid.arrange(gg1, gg2, gg3, gg4, nrow = 2)
```




# Modeling
## Regression Tree

```{r tree}
# Specify CV method
trctrl <- trainControl(method = "LOOCV")

# Normal Regression Tree
treeFit <- train(shares ~., data = train, method = "rpart",
                 trControl=trctrl)
treePred <- predict(treeFit, newdata = test)
treeRMSE <- sqrt(mean((treePred-test$shares)^2))
```
The final regression tree model that I selected was the model with a Cp value of `r round(unname(treeFit$bestTune[1,1]), 5)`.

## Boosted Tree

```{r boost}
# Boosted Regression Tree
trctrl2 <- trainControl(method = "cv", number = 5)
boostFit <- train(shares ~., data = train, method = "gbm",
                  trControl = trctrl2, preProcess = c("center", "scale"),
                  verbose = FALSE)
boostPred <- predict(boostFit, newdata = test)
boostRMSE <- sqrt(mean((boostPred-test$shares)^2))
```

The final boosted tree model that I selected was the model with the following tune of the parameters:
```{r boostbest, echo=FALSE}
boostFit$bestTune
```

## Model Comparison
```{r RMSEtbl, echo=FALSE}
tbl.rmse <- rbind.data.frame("tree" = treeRMSE, "boost" = boostRMSE)
colnames(tbl.rmse) <- "RMSE"
rownames(tbl.rmse) <- c("Reg. Tree", "Boost Tree")
kable(tbl.rmse, caption = "Comparison of Models' RMSE")
```


